/usr/lib/python3/dist-packages/urllib3/util/selectors.py:14: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import namedtuple, Mapping
/usr/lib/python3/dist-packages/urllib3/_collections.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping, MutableMapping
-----------  Configuration Arguments -----------
adam_epsilon: 1e-06
batch_size: 16
device: gpu
layer_lr_decay: 1.0
learning_rate: 1e-05
logging_steps: 100
max_seq_length: 128
max_steps: -1
model_name_or_path: best-mnli_ft_model_37000.pdparams
model_type: mpnet
num_train_epochs: 10
output_dir: mrpc
save_steps: 100
scheduler_type: linear
seed: 42
task_name: mrpc
warmup_proportion: 0.06
warmup_steps: 0
weight_decay: 0.1
------------------------------------------------
2021-08-07 09:02:26,863-INFO: unique_endpoints {'127.0.0.1:33119'}
2021-08-07 09:02:26,864-INFO: unique_endpoints {'127.0.0.1:33119'}
2021-08-07 09:02:26,864-INFO: Downloading dev_ids.tsv from https://dataset.bj.bcebos.com/glue/mrpc/dev_ids.tsv
14.3%28.6%42.9%57.1%71.4%85.7%100.0%
2021-08-07 09:02:27,263-INFO: File /root/.paddlenlp/datasets/Glue/MRPC/dev_ids.tsv md5 checking...
2021-08-07 09:02:27,264-INFO: unique_endpoints {'127.0.0.1:33119'}
2021-08-07 09:02:27,264-INFO: Downloading msr_paraphrase_train.txt from https://dataset.bj.bcebos.com/glue/mrpc/msr_paraphrase_train.txt
0.1%0.2%0.3%0.4%0.5%0.6%0.7%0.8%0.9%1.0%1.1%1.2%1.3%1.4%1.5%1.6%1.7%1.8%1.9%2.0%2.1%2.2%2.2%2.3%2.4%2.5%2.6%2.7%2.8%2.9%3.0%3.1%3.2%3.3%3.4%3.5%3.6%3.7%3.8%3.9%4.0%4.1%4.2%4.3%4.4%4.5%4.6%4.7%4.8%4.9%5.0%5.1%5.2%5.3%5.4%5.5%5.6%5.7%5.8%5.9%6.0%6.1%6.2%6.3%6.4%6.5%6.5%6.6%6.7%6.8%6.9%7.0%7.1%7.2%7.3%7.4%7.5%7.6%7.7%7.8%7.9%8.0%8.1%8.2%8.3%8.4%8.5%8.6%8.7%8.8%8.9%9.0%9.1%9.2%9.3%9.4%9.5%9.6%9.7%9.8%9.9%10.0%10.1%10.2%10.3%10.4%10.5%10.6%10.7%10.8%10.9%10.9%11.0%11.1%11.2%11.3%11.4%11.5%11.6%11.7%11.8%11.9%12.0%12.1%12.2%12.3%12.4%12.5%12.6%12.7%12.8%12.9%13.0%13.1%13.2%13.3%13.4%13.5%13.6%13.7%13.8%13.9%14.0%14.1%14.2%14.3%14.4%14.5%14.6%14.7%14.8%14.9%15.0%15.1%15.2%15.2%15.3%15.4%15.5%15.6%15.7%15.8%15.9%16.0%16.1%16.2%16.3%16.4%16.5%16.6%16.7%16.8%16.9%17.0%17.1%17.2%17.3%17.4%17.5%17.6%17.7%17.8%17.9%18.0%18.1%18.2%18.3%18.4%18.5%18.6%18.7%18.8%18.9%19.0%19.1%19.2%19.3%19.4%19.5%19.6%19.6%19.7%19.8%19.9%20.0%20.1%20.2%20.3%20.4%20.5%20.6%20.7%20.8%20.9%21.0%21.1%21.2%21.3%21.4%21.5%21.6%21.7%21.8%21.9%22.0%22.1%22.2%22.3%22.4%22.5%22.6%22.7%22.8%22.9%23.0%23.1%23.2%23.3%23.4%23.5%23.6%23.7%23.8%23.9%23.9%24.0%24.1%24.2%24.3%24.4%24.5%24.6%24.7%24.8%24.9%25.0%25.1%25.2%25.3%25.4%25.5%25.6%25.7%25.8%25.9%26.0%26.1%26.2%26.3%26.4%26.5%26.6%26.7%26.8%26.9%27.0%27.1%27.2%27.3%27.4%27.5%27.6%27.7%27.8%27.9%28.0%28.1%28.2%28.3%28.3%28.4%28.5%28.6%28.7%28.8%28.9%29.0%29.1%29.2%29.3%29.4%29.5%29.6%29.7%29.8%29.9%30.0%30.1%30.2%30.3%30.4%30.5%30.6%30.7%30.8%30.9%31.0%31.1%31.2%31.3%31.4%31.5%31.6%31.7%31.8%31.9%32.0%32.1%32.2%32.3%32.4%32.5%32.6%32.6%32.7%32.8%32.9%33.0%33.1%33.2%33.3%33.4%33.5%33.6%33.7%33.8%33.9%34.0%34.1%34.2%34.3%34.4%34.5%34.6%34.7%34.8%34.9%35.0%35.1%35.2%35.3%35.4%35.5%35.6%35.7%35.8%35.9%36.0%36.1%36.2%36.3%36.4%36.5%36.6%36.7%36.8%36.9%37.0%37.0%37.1%37.2%37.3%37.4%37.5%37.6%37.7%37.8%37.9%38.0%38.1%38.2%38.3%38.4%38.5%38.6%38.7%38.8%38.9%39.0%39.1%39.2%39.3%39.4%39.5%39.6%39.7%39.8%39.9%40.0%40.1%40.2%40.3%40.4%40.5%40.6%40.7%40.8%40.9%41.0%41.1%41.2%41.3%41.3%41.4%41.5%41.6%41.7%41.8%41.9%42.0%42.1%42.2%42.3%42.4%42.5%42.6%42.7%42.8%42.9%43.0%43.1%43.2%43.3%43.4%43.5%43.6%43.7%43.8%43.9%44.0%44.1%44.2%44.3%44.4%44.5%44.6%44.7%44.8%44.9%45.0%45.1%45.2%45.3%45.4%45.5%45.6%45.7%45.7%45.8%45.9%46.0%46.1%46.2%46.3%46.4%46.5%46.6%46.7%46.8%46.9%47.0%47.1%47.2%47.3%47.4%47.5%47.6%47.7%47.8%47.9%48.0%48.1%48.2%48.3%48.4%48.5%48.6%48.7%48.8%48.9%49.0%49.1%49.2%49.3%49.4%49.5%49.6%49.7%49.8%49.9%50.0%50.0%50.1%50.2%50.3%50.4%50.5%50.6%50.7%50.8%50.9%51.0%51.1%51.2%51.3%51.4%51.5%51.6%51.7%51.8%51.9%52.0%52.1%52.2%52.3%52.4%52.5%52.6%52.7%52.8%52.9%53.0%53.1%53.2%53.3%53.4%53.5%53.6%53.7%53.8%53.9%54.0%54.1%54.2%54.3%54.3%54.4%54.5%54.6%54.7%54.8%54.9%55.0%55.1%55.2%55.3%55.4%55.5%55.6%55.7%55.8%55.9%56.0%56.1%56.2%56.3%56.4%56.5%56.6%56.7%56.8%56.9%57.0%57.1%57.2%57.3%57.4%57.5%57.6%57.7%57.8%57.9%58.0%58.1%58.2%58.3%58.4%58.5%58.6%58.7%58.7%58.8%58.9%59.0%59.1%59.2%59.3%59.4%59.5%59.6%59.7%59.8%59.9%60.0%60.1%60.2%60.3%60.4%60.5%60.6%60.7%60.8%60.9%61.0%61.1%61.2%61.3%61.4%61.5%61.6%61.7%61.8%61.9%62.0%62.1%62.2%62.3%62.4%62.5%62.6%62.7%62.8%62.9%63.0%63.0%63.1%63.2%63.3%63.4%63.5%63.6%63.7%63.8%63.9%64.0%64.1%64.2%64.3%64.4%64.5%64.6%64.7%64.8%64.9%65.0%65.1%65.2%65.3%65.4%65.5%65.6%65.7%65.8%65.9%66.0%66.1%66.2%66.3%66.4%66.5%66.6%66.7%66.8%66.9%67.0%67.1%67.2%67.3%67.4%67.4%67.5%67.6%67.7%67.8%67.9%68.0%68.1%68.2%68.3%68.4%68.5%68.6%68.7%68.8%68.9%69.0%69.1%69.2%69.3%69.4%69.5%69.6%69.7%69.8%69.9%70.0%70.1%70.2%70.3%70.4%70.5%70.6%70.7%70.8%70.9%71.0%71.1%71.2%71.3%71.4%71.5%71.6%71.7%71.7%71.8%71.9%72.0%72.1%72.2%72.3%72.4%72.5%72.6%72.7%72.8%72.9%73.0%73.1%73.2%73.3%73.4%73.5%73.6%73.7%73.8%73.9%74.0%74.1%74.2%74.3%74.4%74.5%74.6%74.7%74.8%74.9%75.0%75.1%75.2%75.3%75.4%75.5%75.6%75.7%75.8%75.9%76.0%76.1%76.1%76.2%76.3%76.4%76.5%76.6%76.7%76.8%76.9%77.0%77.1%77.2%77.3%77.4%77.5%77.6%77.7%77.8%77.9%78.0%78.1%78.2%78.3%78.4%78.5%78.6%78.7%78.8%78.9%79.0%79.1%79.2%79.3%79.4%79.5%79.6%79.7%79.8%79.9%80.0%80.1%80.2%80.3%80.4%80.4%80.5%80.6%80.7%80.8%80.9%81.0%81.1%81.2%81.3%81.4%81.5%81.6%81.7%81.8%81.9%82.0%82.1%82.2%82.3%82.4%82.5%82.6%82.7%82.8%82.9%83.0%83.1%83.2%83.3%83.4%83.5%83.6%83.7%83.8%83.9%84.0%84.1%84.2%84.3%84.4%84.5%84.6%84.7%84.8%84.8%84.9%85.0%85.1%85.2%85.3%85.4%85.5%85.6%85.7%85.8%85.9%86.0%86.1%86.2%86.3%86.4%86.5%86.6%86.7%86.8%86.9%87.0%87.1%87.2%87.3%87.4%87.5%87.6%87.7%87.8%87.9%88.0%88.1%88.2%88.3%88.4%88.5%88.6%88.7%88.8%88.9%89.0%89.1%89.1%89.2%89.3%89.4%89.5%89.6%89.7%89.8%89.9%90.0%90.1%90.2%90.3%90.4%90.5%90.6%90.7%90.8%90.9%91.0%91.1%91.2%91.3%91.4%91.5%91.6%91.7%91.8%91.9%92.0%92.1%92.2%92.3%92.4%92.5%92.6%92.7%92.8%92.9%93.0%93.1%93.2%93.3%93.4%93.5%93.5%93.6%93.7%93.8%93.9%94.0%94.1%94.2%94.3%94.4%94.5%94.6%94.7%94.8%94.9%95.0%95.1%95.2%95.3%95.4%95.5%95.6%95.7%95.8%95.9%96.0%96.1%96.2%96.3%96.4%96.5%96.6%96.7%96.8%96.9%97.0%97.1%97.2%97.3%97.4%97.5%97.6%97.7%97.8%97.8%97.9%98.0%98.1%98.2%98.3%98.4%98.5%98.6%98.7%98.8%98.9%99.0%99.1%99.2%99.3%99.4%99.5%99.6%99.7%99.8%99.9%100.0%
2021-08-07 09:02:28,206-INFO: File /root/.paddlenlp/datasets/Glue/MRPC/msr_paraphrase_train.txt md5 checking...
/root/.paddlenlp/datasets/Glue/MRPC/dev_ids.tsv
2021-08-07 09:02:28,299-INFO: unique_endpoints {'127.0.0.1:33119'}
W0807 09:02:28.302071   955 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.4, Runtime API Version: 11.2
W0807 09:02:28.304286   955 device_context.cc:422] device: 0, cuDNN Version: 8.1.
num_training_steps 2300
global step 100/2300, epoch: 0, batch: 99, rank_id: 0, loss: 0.502403, lr: 0.0000072464, speed: 12.9510 step/s
====================================================================================================
eval loss: 0.390765, acc: 0.8088235294117647, precision: 0.8092307692307692, recall: 0.942652329749104, f1: 0.870860927152318, acc and f1: 0.8398422282820414, 
eval done total : 0.5429003238677979 s
global step 200/2300, epoch: 0, batch: 199, rank_id: 0, loss: 0.360718, lr: 0.0000097132, speed: 8.8731 step/s
====================================================================================================
eval loss: 0.218807, acc: 0.8504901960784313, precision: 0.8538961038961039, recall: 0.942652329749104, f1: 0.8960817717206134, acc and f1: 0.8732859838995224, 
eval done total : 0.5076279640197754 s
global step 300/2300, epoch: 1, batch: 69, rank_id: 0, loss: 0.104458, lr: 0.0000092507, speed: 8.8582 step/s
====================================================================================================
eval loss: 0.152877, acc: 0.8553921568627451, precision: 0.8594771241830066, recall: 0.942652329749104, f1: 0.8991452991452993, acc and f1: 0.8772687280040221, 
eval done total : 0.5260055065155029 s
global step 400/2300, epoch: 1, batch: 169, rank_id: 0, loss: 0.238771, lr: 0.0000087882, speed: 8.7779 step/s
====================================================================================================
eval loss: 0.168062, acc: 0.875, precision: 0.8931034482758621, recall: 0.9283154121863799, f1: 0.9103690685413005, acc and f1: 0.8926845342706502, 
eval done total : 0.5250973701477051 s
global step 500/2300, epoch: 2, batch: 39, rank_id: 0, loss: 0.149117, lr: 0.0000083256, speed: 9.1492 step/s
====================================================================================================
eval loss: 0.106607, acc: 0.8799019607843137, precision: 0.9078014184397163, recall: 0.9175627240143369, f1: 0.9126559714795008, acc and f1: 0.8962789661319073, 
eval done total : 0.5184292793273926 s
global step 600/2300, epoch: 2, batch: 139, rank_id: 0, loss: 0.129740, lr: 0.0000078631, speed: 9.0326 step/s
====================================================================================================
eval loss: 0.107050, acc: 0.8823529411764706, precision: 0.9110320284697508, recall: 0.9175627240143369, f1: 0.9142857142857143, acc and f1: 0.8983193277310924, 
eval done total : 0.5295462608337402 s
global step 700/2300, epoch: 3, batch: 9, rank_id: 0, loss: 0.058137, lr: 0.0000074006, speed: 9.2143 step/s
====================================================================================================
eval loss: 0.063322, acc: 0.8897058823529411, precision: 0.9239130434782609, recall: 0.9139784946236559, f1: 0.918918918918919, acc and f1: 0.90431240063593, 
eval done total : 0.5544917583465576 s
global step 800/2300, epoch: 3, batch: 109, rank_id: 0, loss: 0.025706, lr: 0.0000069380, speed: 9.1169 step/s
====================================================================================================
eval loss: 0.059215, acc: 0.8921568627450981, precision: 0.9304029304029304, recall: 0.910394265232975, f1: 0.9202898550724639, acc and f1: 0.9062233589087809, 
eval done total : 0.5176815986633301 s
global step 900/2300, epoch: 3, batch: 209, rank_id: 0, loss: 0.172971, lr: 0.0000064755, speed: 9.0620 step/s
====================================================================================================
eval loss: 0.088343, acc: 0.8995098039215687, precision: 0.9375, recall: 0.9139784946236559, f1: 0.925589836660617, acc and f1: 0.9125498202910929, 
eval done total : 0.5385515689849854 s
global step 1000/2300, epoch: 4, batch: 79, rank_id: 0, loss: 0.179410, lr: 0.0000060130, speed: 9.0572 step/s
====================================================================================================
eval loss: 0.041874, acc: 0.8823529411764706, precision: 0.8915254237288136, recall: 0.942652329749104, f1: 0.9163763066202091, acc and f1: 0.8993646238983398, 
eval done total : 0.5344977378845215 s
global step 1100/2300, epoch: 4, batch: 179, rank_id: 0, loss: 0.171355, lr: 0.0000055504, speed: 9.2528 step/s
====================================================================================================
eval loss: 0.034141, acc: 0.8799019607843137, precision: 0.8833333333333333, recall: 0.9498207885304659, f1: 0.9153713298791019, acc and f1: 0.8976366453317077, 
eval done total : 0.5236728191375732 s
global step 1200/2300, epoch: 5, batch: 49, rank_id: 0, loss: 0.013998, lr: 0.0000050879, speed: 9.2611 step/s
====================================================================================================
eval loss: 0.083660, acc: 0.8872549019607843, precision: 0.8949152542372881, recall: 0.946236559139785, f1: 0.9198606271777002, acc and f1: 0.9035577645692423, 
eval done total : 0.5335831642150879 s
global step 1300/2300, epoch: 5, batch: 149, rank_id: 0, loss: 0.013235, lr: 0.0000046253, speed: 9.4321 step/s
====================================================================================================
eval loss: 0.040779, acc: 0.9019607843137255, precision: 0.9283154121863799, recall: 0.9283154121863799, f1: 0.9283154121863799, acc and f1: 0.9151380982500528, 
eval done total : 0.5108661651611328 s
global step 1400/2300, epoch: 6, batch: 19, rank_id: 0, loss: 0.011537, lr: 0.0000041628, speed: 9.4161 step/s
====================================================================================================
eval loss: 0.028035, acc: 0.8970588235294118, precision: 0.9100346020761245, recall: 0.942652329749104, f1: 0.926056338028169, acc and f1: 0.9115575807787903, 
eval done total : 0.5299506187438965 s
global step 1500/2300, epoch: 6, batch: 119, rank_id: 0, loss: 0.013453, lr: 0.0000037003, speed: 9.3623 step/s
====================================================================================================
eval loss: 0.015124, acc: 0.8848039215686274, precision: 0.8918918918918919, recall: 0.946236559139785, f1: 0.9182608695652175, acc and f1: 0.9015323955669224, 
eval done total : 0.5263650417327881 s
global step 1600/2300, epoch: 6, batch: 219, rank_id: 0, loss: 0.010005, lr: 0.0000032377, speed: 9.3700 step/s
====================================================================================================
eval loss: 0.013289, acc: 0.9044117647058824, precision: 0.9195804195804196, recall: 0.942652329749104, f1: 0.9309734513274337, acc and f1: 0.9176926080166581, 
eval done total : 0.5342140197753906 s
global step 1700/2300, epoch: 7, batch: 89, rank_id: 0, loss: 0.009847, lr: 0.0000027752, speed: 9.0372 step/s
====================================================================================================
eval loss: 0.031101, acc: 0.9019607843137255, precision: 0.9222614840989399, recall: 0.9354838709677419, f1: 0.9288256227758006, acc and f1: 0.9153932035447631, 
eval done total : 0.5342972278594971 s
global step 1800/2300, epoch: 7, batch: 189, rank_id: 0, loss: 0.010664, lr: 0.0000023127, speed: 9.2223 step/s
====================================================================================================
eval loss: 0.021424, acc: 0.8921568627450981, precision: 0.9065743944636678, recall: 0.9390681003584229, f1: 0.9225352112676057, acc and f1: 0.907346037006352, 
eval done total : 0.5174486637115479 s
global step 1900/2300, epoch: 8, batch: 59, rank_id: 0, loss: 0.013202, lr: 0.0000018501, speed: 9.3530 step/s
====================================================================================================
eval loss: 0.026341, acc: 0.8970588235294118, precision: 0.9157894736842105, recall: 0.9354838709677419, f1: 0.925531914893617, acc and f1: 0.9112953692115144, 
eval done total : 0.535048246383667 s
global step 2000/2300, epoch: 8, batch: 159, rank_id: 0, loss: 0.363392, lr: 0.0000013876, speed: 9.2546 step/s
====================================================================================================
eval loss: 0.023188, acc: 0.8970588235294118, precision: 0.9187279151943463, recall: 0.931899641577061, f1: 0.9252669039145908, acc and f1: 0.9111628637220013, 
eval done total : 0.5497183799743652 s
global step 2100/2300, epoch: 9, batch: 29, rank_id: 0, loss: 0.007734, lr: 0.0000009251, speed: 9.4073 step/s
====================================================================================================
eval loss: 0.014245, acc: 0.9019607843137255, precision: 0.9106529209621993, recall: 0.9498207885304659, f1: 0.9298245614035087, acc and f1: 0.9158926728586171, 
eval done total : 0.5383467674255371 s
global step 2200/2300, epoch: 9, batch: 129, rank_id: 0, loss: 0.053527, lr: 0.0000004625, speed: 9.2686 step/s
====================================================================================================
eval loss: 0.019149, acc: 0.9019607843137255, precision: 0.9252669039145908, recall: 0.931899641577061, f1: 0.9285714285714286, acc and f1: 0.9152661064425771, 
eval done total : 0.5251131057739258 s
global step 2300/2300, epoch: 9, batch: 229, rank_id: 0, loss: 0.015801, lr: 0.0000000000, speed: 9.4603 step/s
====================================================================================================
eval loss: 0.014990, acc: 0.8995098039215687, precision: 0.9131944444444444, recall: 0.942652329749104, f1: 0.927689594356261, acc and f1: 0.9135996991389148, 
eval done total : 0.5274949073791504 s

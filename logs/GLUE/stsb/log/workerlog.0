/usr/lib/python3/dist-packages/urllib3/util/selectors.py:14: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import namedtuple, Mapping
/usr/lib/python3/dist-packages/urllib3/_collections.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping, MutableMapping
-----------  Configuration Arguments -----------
adam_epsilon: 1e-06
batch_size: 16
device: gpu
layer_lr_decay: 1.0
learning_rate: 2e-05
logging_steps: 100
max_seq_length: 128
max_steps: -1
model_name_or_path: best-mnli_ft_model_37000.pdparams
model_type: mpnet
num_train_epochs: 10
output_dir: sts-b
save_steps: 100
scheduler_type: linear
seed: 42
task_name: sts-b
warmup_proportion: 0.06
warmup_steps: 0
weight_decay: 0.1
------------------------------------------------
2021-08-07 09:07:47,996-INFO: unique_endpoints {'127.0.0.1:59119'}
2021-08-07 09:07:47,996-INFO: unique_endpoints {'127.0.0.1:59119'}
2021-08-07 09:07:47,996-INFO: Downloading STS.zip from https://dataset.bj.bcebos.com/glue/STS.zip
0.1%0.3%0.4%0.5%0.6%0.8%0.9%1.0%1.1%1.3%1.4%1.5%1.7%1.8%1.9%2.0%2.2%2.3%2.4%2.5%2.7%2.8%2.9%3.1%3.2%3.3%3.4%3.6%3.7%3.8%3.9%4.1%4.2%4.3%4.5%4.6%4.7%4.8%5.0%5.1%5.2%5.4%5.5%5.6%5.7%5.9%6.0%6.1%6.2%6.4%6.5%6.6%6.8%6.9%7.0%7.1%7.3%7.4%7.5%7.6%7.8%7.9%8.0%8.2%8.3%8.4%8.5%8.7%8.8%8.9%9.0%9.2%9.3%9.4%9.6%9.7%9.8%9.9%10.1%10.2%10.3%10.4%10.6%10.7%10.8%11.0%11.1%11.2%11.3%11.5%11.6%11.7%11.8%12.0%12.1%12.2%12.4%12.5%12.6%12.7%12.9%13.0%13.1%13.2%13.4%13.5%13.6%13.8%13.9%14.0%14.1%14.3%14.4%14.5%14.6%14.8%14.9%15.0%15.2%15.3%15.4%15.5%15.7%15.8%15.9%16.1%16.2%16.3%16.4%16.6%16.7%16.8%16.9%17.1%17.2%17.3%17.5%17.6%17.7%17.8%18.0%18.1%18.2%18.3%18.5%18.6%18.7%18.9%19.0%19.1%19.2%19.4%19.5%19.6%19.7%19.9%20.0%20.1%20.3%20.4%20.5%20.6%20.8%20.9%21.0%21.1%21.3%21.4%21.5%21.7%21.8%21.9%22.0%22.2%22.3%22.4%22.5%22.7%22.8%22.9%23.1%23.2%23.3%23.4%23.6%23.7%23.8%23.9%24.1%24.2%24.3%24.5%24.6%24.7%24.8%25.0%25.1%25.2%25.4%25.5%25.6%25.7%25.9%26.0%26.1%26.2%26.4%26.5%26.6%26.8%26.9%27.0%27.1%27.3%27.4%27.5%27.6%27.8%27.9%28.0%28.2%28.3%28.4%28.5%28.7%28.8%28.9%29.0%29.2%29.3%29.4%29.6%29.7%29.8%29.9%30.1%30.2%30.3%30.4%30.6%30.7%30.8%31.0%31.1%31.2%31.3%31.5%31.6%31.7%31.8%32.0%32.1%32.2%32.4%32.5%32.6%32.7%32.9%33.0%33.1%33.2%33.4%33.5%33.6%33.8%33.9%34.0%34.1%34.3%34.4%34.5%34.6%34.8%34.9%35.0%35.2%35.3%35.4%35.5%35.7%35.8%35.9%36.1%36.2%36.3%36.4%36.6%36.7%36.8%36.9%37.1%37.2%37.3%37.5%37.6%37.7%37.8%38.0%38.1%38.2%38.3%38.5%38.6%38.7%38.9%39.0%39.1%39.2%39.4%39.5%39.6%39.7%39.9%40.0%40.1%40.3%40.4%40.5%40.6%40.8%40.9%41.0%41.1%41.3%41.4%41.5%41.7%41.8%41.9%42.0%42.2%42.3%42.4%42.5%42.7%42.8%42.9%43.1%43.2%43.3%43.4%43.6%43.7%43.8%43.9%44.1%44.2%44.3%44.5%44.6%44.7%44.8%45.0%45.1%45.2%45.4%45.5%45.6%45.7%45.9%46.0%46.1%46.2%46.4%46.5%46.6%46.8%46.9%47.0%47.1%47.3%47.4%47.5%47.6%47.8%47.9%48.0%48.2%48.3%48.4%48.5%48.7%48.8%48.9%49.0%49.2%49.3%49.4%49.6%49.7%49.8%49.9%50.1%50.2%50.3%50.4%50.6%50.7%50.8%51.0%51.1%51.2%51.3%51.5%51.6%51.7%51.8%52.0%52.1%52.2%52.4%52.5%52.6%52.7%52.9%53.0%53.1%53.2%53.4%53.5%53.6%53.8%53.9%54.0%54.1%54.3%54.4%54.5%54.6%54.8%54.9%55.0%55.2%55.3%55.4%55.5%55.7%55.8%55.9%56.1%56.2%56.3%56.4%56.6%56.7%56.8%56.9%57.1%57.2%57.3%57.5%57.6%57.7%57.8%58.0%58.1%58.2%58.3%58.5%58.6%58.7%58.9%59.0%59.1%59.2%59.4%59.5%59.6%59.7%59.9%60.0%60.1%60.3%60.4%60.5%60.6%60.8%60.9%61.0%61.1%61.3%61.4%61.5%61.7%61.8%61.9%62.0%62.2%62.3%62.4%62.5%62.7%62.8%62.9%63.1%63.2%63.3%63.4%63.6%63.7%63.8%63.9%64.1%64.2%64.3%64.5%64.6%64.7%64.8%65.0%65.1%65.2%65.4%65.5%65.6%65.7%65.9%66.0%66.1%66.2%66.4%66.5%66.6%66.8%66.9%67.0%67.1%67.3%67.4%67.5%67.6%67.8%67.9%68.0%68.2%68.3%68.4%68.5%68.7%68.8%68.9%69.0%69.2%69.3%69.4%69.6%69.7%69.8%69.9%70.1%70.2%70.3%70.4%70.6%70.7%70.8%71.0%71.1%71.2%71.3%71.5%71.6%71.7%71.8%72.0%72.1%72.2%72.4%72.5%72.6%72.7%72.9%73.0%73.1%73.2%73.4%73.5%73.6%73.8%73.9%74.0%74.1%74.3%74.4%74.5%74.6%74.8%74.9%75.0%75.2%75.3%75.4%75.5%75.7%75.8%75.9%76.1%76.2%76.3%76.4%76.6%76.7%76.8%76.9%77.1%77.2%77.3%77.5%77.6%77.7%77.8%78.0%78.1%78.2%78.3%78.5%78.6%78.7%78.9%79.0%79.1%79.2%79.4%79.5%79.6%79.7%79.9%80.0%80.1%80.3%80.4%80.5%80.6%80.8%80.9%81.0%81.1%81.3%81.4%81.5%81.7%81.8%81.9%82.0%82.2%82.3%82.4%82.5%82.7%82.8%82.9%83.1%83.2%83.3%83.4%83.6%83.7%83.8%83.9%84.1%84.2%84.3%84.5%84.6%84.7%84.8%85.0%85.1%85.2%85.4%85.5%85.6%85.7%85.9%86.0%86.1%86.2%86.4%86.5%86.6%86.8%86.9%87.0%87.1%87.3%87.4%87.5%87.6%87.8%87.9%88.0%88.2%88.3%88.4%88.5%88.7%88.8%88.9%89.0%89.2%89.3%89.4%89.6%89.7%89.8%89.9%90.1%90.2%90.3%90.4%90.6%90.7%90.8%91.0%91.1%91.2%91.3%91.5%91.6%91.7%91.8%92.0%92.1%92.2%92.4%92.5%92.6%92.7%92.9%93.0%93.1%93.2%93.4%93.5%93.6%93.8%93.9%94.0%94.1%94.3%94.4%94.5%94.6%94.8%94.9%95.0%95.2%95.3%95.4%95.5%95.7%95.8%95.9%96.1%96.2%96.3%96.4%96.6%96.7%96.8%96.9%97.1%97.2%97.3%97.5%97.6%97.7%97.8%98.0%98.1%98.2%98.3%98.5%98.6%98.7%98.9%99.0%99.1%99.2%99.4%99.5%99.6%99.7%99.9%100.0%
2021-08-07 09:07:48,982-INFO: File /root/.paddlenlp/datasets/Glue/STS.zip md5 checking...
2021-08-07 09:07:48,986-INFO: Decompressing /root/.paddlenlp/datasets/Glue/STS.zip...
2021-08-07 09:07:49,030-INFO: unique_endpoints {'127.0.0.1:59119'}
W0807 09:07:49.035395  1296 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.4, Runtime API Version: 11.2
W0807 09:07:49.037709  1296 device_context.cc:422] device: 0, cuDNN Version: 8.1.
num_training_steps 3600
global step 100/3600, epoch: 0, batch: 99, rank_id: 0, loss: 4.168812, lr: 0.0000092593, speed: 13.7280 step/s
====================================================================================================
eval loss: 4.534630, pearson: 0.22825903823974372, spearman: 0.5938752150556512, pearson and spearman: 0.41106712664769746, eval done total : 1.169954776763916 s
global step 200/3600, epoch: 0, batch: 199, rank_id: 0, loss: 0.877686, lr: 0.0000185185, speed: 8.9755 step/s
====================================================================================================
eval loss: 1.387904, pearson: 0.7856709961658962, spearman: 0.8424079086257372, pearson and spearman: 0.8140394523958168, eval done total : 1.2261652946472168 s
global step 300/3600, epoch: 0, batch: 299, rank_id: 0, loss: 0.460170, lr: 0.0000195035, speed: 8.4858 step/s
====================================================================================================
eval loss: 0.741829, pearson: 0.8739723793111979, spearman: 0.8731581160702738, pearson and spearman: 0.8735652476907358, eval done total : 1.180896520614624 s
global step 400/3600, epoch: 1, batch: 39, rank_id: 0, loss: 0.421297, lr: 0.0000189125, speed: 8.8387 step/s
====================================================================================================
eval loss: 0.708364, pearson: 0.8984405631437936, spearman: 0.9008104341379707, pearson and spearman: 0.8996254986408821, eval done total : 1.2173194885253906 s
global step 500/3600, epoch: 1, batch: 139, rank_id: 0, loss: 0.552125, lr: 0.0000183215, speed: 8.9626 step/s
====================================================================================================
eval loss: 0.600327, pearson: 0.9040155291445144, spearman: 0.9051434458415315, pearson and spearman: 0.904579487493023, eval done total : 1.1928901672363281 s
global step 600/3600, epoch: 1, batch: 239, rank_id: 0, loss: 0.541609, lr: 0.0000177305, speed: 8.6811 step/s
====================================================================================================
eval loss: 0.576527, pearson: 0.8970831834274406, spearman: 0.9001019573786477, pearson and spearman: 0.8985925704030442, eval done total : 1.2423896789550781 s
global step 700/3600, epoch: 1, batch: 339, rank_id: 0, loss: 0.357182, lr: 0.0000171395, speed: 8.8307 step/s
====================================================================================================
eval loss: 0.621552, pearson: 0.9114032151888694, spearman: 0.9099914764406563, pearson and spearman: 0.9106973458147628, eval done total : 1.2079269886016846 s
global step 800/3600, epoch: 2, batch: 79, rank_id: 0, loss: 0.201150, lr: 0.0000165485, speed: 9.0118 step/s
====================================================================================================
eval loss: 0.629945, pearson: 0.9135124434194047, spearman: 0.9126911291960574, pearson and spearman: 0.9131017863077311, eval done total : 1.2015671730041504 s
global step 900/3600, epoch: 2, batch: 179, rank_id: 0, loss: 0.235921, lr: 0.0000159574, speed: 8.9005 step/s
====================================================================================================
eval loss: 0.647151, pearson: 0.9092257454563757, spearman: 0.9109497026443123, pearson and spearman: 0.910087724050344, eval done total : 1.1916356086730957 s
global step 1000/3600, epoch: 2, batch: 279, rank_id: 0, loss: 0.355675, lr: 0.0000153664, speed: 8.9381 step/s
====================================================================================================
eval loss: 0.516223, pearson: 0.9145972913272831, spearman: 0.9119475341989041, pearson and spearman: 0.9132724127630936, eval done total : 1.1889643669128418 s
global step 1100/3600, epoch: 3, batch: 19, rank_id: 0, loss: 0.155760, lr: 0.0000147754, speed: 8.8947 step/s
====================================================================================================
eval loss: 0.684359, pearson: 0.9147946698106434, spearman: 0.912069251586334, pearson and spearman: 0.9134319606984886, eval done total : 1.2024075984954834 s
global step 1200/3600, epoch: 3, batch: 119, rank_id: 0, loss: 0.233245, lr: 0.0000141844, speed: 8.8250 step/s
====================================================================================================
eval loss: 0.548975, pearson: 0.9075348094573873, spearman: 0.908916581740703, pearson and spearman: 0.9082256955990451, eval done total : 1.2335951328277588 s
global step 1300/3600, epoch: 3, batch: 219, rank_id: 0, loss: 0.293533, lr: 0.0000135934, speed: 8.9395 step/s
====================================================================================================
eval loss: 0.501704, pearson: 0.9145409658813237, spearman: 0.9125602029156458, pearson and spearman: 0.9135505843984848, eval done total : 1.2051854133605957 s
global step 1400/3600, epoch: 3, batch: 319, rank_id: 0, loss: 0.103747, lr: 0.0000130024, speed: 8.9393 step/s
====================================================================================================
eval loss: 0.655761, pearson: 0.9139649298542127, spearman: 0.9114333188592528, pearson and spearman: 0.9126991243567328, eval done total : 1.1853885650634766 s
global step 1500/3600, epoch: 4, batch: 59, rank_id: 0, loss: 0.133028, lr: 0.0000124113, speed: 8.9844 step/s
====================================================================================================
eval loss: 0.553544, pearson: 0.9159433773878062, spearman: 0.9132826636811838, pearson and spearman: 0.914613020534495, eval done total : 1.1956062316894531 s
global step 1600/3600, epoch: 4, batch: 159, rank_id: 0, loss: 0.114247, lr: 0.0000118203, speed: 8.9944 step/s
====================================================================================================
eval loss: 0.670653, pearson: 0.9126001127292912, spearman: 0.9124810206582314, pearson and spearman: 0.9125405666937614, eval done total : 1.2353088855743408 s
global step 1700/3600, epoch: 4, batch: 259, rank_id: 0, loss: 0.067095, lr: 0.0000112293, speed: 8.6841 step/s
====================================================================================================
eval loss: 0.638566, pearson: 0.9139841142114903, spearman: 0.9122579236701883, pearson and spearman: 0.9131210189408393, eval done total : 1.191378116607666 s
global step 1800/3600, epoch: 4, batch: 359, rank_id: 0, loss: 0.178605, lr: 0.0000106383, speed: 8.9593 step/s
====================================================================================================
eval loss: 0.573159, pearson: 0.9149603266231355, spearman: 0.9126191789418573, pearson and spearman: 0.9137897527824964, eval done total : 1.18017578125 s
global step 1900/3600, epoch: 5, batch: 99, rank_id: 0, loss: 0.099693, lr: 0.0000100473, speed: 8.8533 step/s
====================================================================================================
eval loss: 0.682828, pearson: 0.9102431710446406, spearman: 0.9087478598879377, pearson and spearman: 0.9094955154662892, eval done total : 1.2709283828735352 s
global step 2000/3600, epoch: 5, batch: 199, rank_id: 0, loss: 0.067986, lr: 0.0000094563, speed: 8.7010 step/s
====================================================================================================
eval loss: 0.521361, pearson: 0.9148330401811303, spearman: 0.9125430366857941, pearson and spearman: 0.9136880384334622, eval done total : 1.23624849319458 s
global step 2100/3600, epoch: 5, batch: 299, rank_id: 0, loss: 0.073403, lr: 0.0000088652, speed: 8.7970 step/s
====================================================================================================
eval loss: 0.664264, pearson: 0.9137742914266758, spearman: 0.9110174715633207, pearson and spearman: 0.9123958814949982, eval done total : 1.206831693649292 s
global step 2200/3600, epoch: 6, batch: 39, rank_id: 0, loss: 0.034356, lr: 0.0000082742, speed: 8.9695 step/s
====================================================================================================
eval loss: 0.564722, pearson: 0.9123273534399292, spearman: 0.9105591495818443, pearson and spearman: 0.9114432515108868, eval done total : 1.2195703983306885 s
global step 2300/3600, epoch: 6, batch: 139, rank_id: 0, loss: 0.084007, lr: 0.0000076832, speed: 8.9406 step/s
====================================================================================================
eval loss: 0.620761, pearson: 0.9146135500268231, spearman: 0.913740793662575, pearson and spearman: 0.914177171844699, eval done total : 1.1886861324310303 s
global step 2400/3600, epoch: 6, batch: 239, rank_id: 0, loss: 0.064840, lr: 0.0000070922, speed: 8.9326 step/s
====================================================================================================
eval loss: 0.527864, pearson: 0.9149907874065636, spearman: 0.9127110047604465, pearson and spearman: 0.913850896083505, eval done total : 1.1751418113708496 s
global step 2500/3600, epoch: 6, batch: 339, rank_id: 0, loss: 0.130404, lr: 0.0000065012, speed: 8.9663 step/s
====================================================================================================
eval loss: 0.669872, pearson: 0.9119158855929664, spearman: 0.9109050804022579, pearson and spearman: 0.9114104829976122, eval done total : 1.203986644744873 s
global step 2600/3600, epoch: 7, batch: 79, rank_id: 0, loss: 0.061745, lr: 0.0000059102, speed: 8.9296 step/s
====================================================================================================
eval loss: 0.595794, pearson: 0.9128312950729894, spearman: 0.9110328102368046, pearson and spearman: 0.9119320526548971, eval done total : 1.1683740615844727 s
global step 2700/3600, epoch: 7, batch: 179, rank_id: 0, loss: 0.081611, lr: 0.0000053191, speed: 9.0332 step/s
====================================================================================================
eval loss: 0.553286, pearson: 0.9130762741780462, spearman: 0.9114184352970823, pearson and spearman: 0.9122473547375642, eval done total : 1.195096731185913 s
global step 2800/3600, epoch: 7, batch: 279, rank_id: 0, loss: 0.118377, lr: 0.0000047281, speed: 8.9425 step/s
====================================================================================================
eval loss: 0.588841, pearson: 0.913691166500992, spearman: 0.9110836262593894, pearson and spearman: 0.9123873963801907, eval done total : 1.190666913986206 s
global step 2900/3600, epoch: 8, batch: 19, rank_id: 0, loss: 0.036099, lr: 0.0000041371, speed: 9.0466 step/s
====================================================================================================
eval loss: 0.555616, pearson: 0.9146191405266948, spearman: 0.913277141900952, pearson and spearman: 0.9139481412138234, eval done total : 1.1895747184753418 s
global step 3000/3600, epoch: 8, batch: 119, rank_id: 0, loss: 0.035052, lr: 0.0000035461, speed: 9.1107 step/s
====================================================================================================
eval loss: 0.587141, pearson: 0.9142090970747709, spearman: 0.9120861262605006, pearson and spearman: 0.9131476116676358, eval done total : 1.2036619186401367 s
global step 3100/3600, epoch: 8, batch: 219, rank_id: 0, loss: 0.070099, lr: 0.0000029551, speed: 9.0073 step/s
====================================================================================================
eval loss: 0.523002, pearson: 0.914161778395085, spearman: 0.9121094578264257, pearson and spearman: 0.9131356181107553, eval done total : 1.1891660690307617 s
global step 3200/3600, epoch: 8, batch: 319, rank_id: 0, loss: 0.058199, lr: 0.0000023641, speed: 8.9999 step/s
====================================================================================================
eval loss: 0.574947, pearson: 0.9152069734280601, spearman: 0.9119890488840218, pearson and spearman: 0.9135980111560409, eval done total : 1.1930654048919678 s
global step 3300/3600, epoch: 9, batch: 59, rank_id: 0, loss: 0.069504, lr: 0.0000017730, speed: 8.9818 step/s
====================================================================================================
eval loss: 0.560397, pearson: 0.9143361739069497, spearman: 0.9117748123443611, pearson and spearman: 0.9130554931256554, eval done total : 1.1835787296295166 s
global step 3400/3600, epoch: 9, batch: 159, rank_id: 0, loss: 0.056029, lr: 0.0000011820, speed: 8.9018 step/s
====================================================================================================
eval loss: 0.582174, pearson: 0.9142086789891831, spearman: 0.9114258699670533, pearson and spearman: 0.9128172744781182, eval done total : 1.2096707820892334 s
global step 3500/3600, epoch: 9, batch: 259, rank_id: 0, loss: 0.050944, lr: 0.0000005910, speed: 9.0295 step/s
====================================================================================================
eval loss: 0.539866, pearson: 0.914471257752072, spearman: 0.9121977671990077, pearson and spearman: 0.9133345124755399, eval done total : 1.178035020828247 s
global step 3600/3600, epoch: 9, batch: 359, rank_id: 0, loss: 0.029431, lr: 0.0000000000, speed: 9.1190 step/s
====================================================================================================
eval loss: 0.562867, pearson: 0.9143926798815187, spearman: 0.9119999644444287, pearson and spearman: 0.9131963221629738, eval done total : 1.1695549488067627 s

/usr/lib/python3/dist-packages/urllib3/util/selectors.py:14: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import namedtuple, Mapping
/usr/lib/python3/dist-packages/urllib3/_collections.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping, MutableMapping
-----------  Configuration Arguments -----------
adam_epsilon: 1e-06
batch_size: 16
device: gpu
layer_lr_decay: 1.0
learning_rate: 2e-05
logging_steps: 100
max_seq_length: 128
max_steps: -1
model_name_or_path: best-mnli_ft_model_37000.pdparams
model_type: mpnet
num_train_epochs: 10
output_dir: rte
save_steps: 100
scheduler_type: linear
seed: 42
task_name: rte
warmup_proportion: 0.06
warmup_steps: 0
weight_decay: 0.1
------------------------------------------------
2021-08-07 08:58:37,949-INFO: unique_endpoints {'127.0.0.1:54561'}
2021-08-07 08:58:37,949-INFO: unique_endpoints {'127.0.0.1:54561'}
2021-08-07 08:58:37,950-INFO: Downloading RTE.zip from https://dataset.bj.bcebos.com/glue/RTE.zip
0.1%0.3%0.4%0.6%0.7%0.9%1.0%1.2%1.3%1.5%1.6%1.8%1.9%2.1%2.2%2.3%2.5%2.6%2.8%2.9%3.1%3.2%3.4%3.5%3.7%3.8%4.0%4.1%4.3%4.4%4.6%4.7%4.8%5.0%5.1%5.3%5.4%5.6%5.7%5.9%6.0%6.2%6.3%6.5%6.6%6.8%6.9%7.0%7.2%7.3%7.5%7.6%7.8%7.9%8.1%8.2%8.4%8.5%8.7%8.8%9.0%9.1%9.3%9.4%9.5%9.7%9.8%10.0%10.1%10.3%10.4%10.6%10.7%10.9%11.0%11.2%11.3%11.5%11.6%11.7%11.9%12.0%12.2%12.3%12.5%12.6%12.8%12.9%13.1%13.2%13.4%13.5%13.7%13.8%14.0%14.1%14.2%14.4%14.5%14.7%14.8%15.0%15.1%15.3%15.4%15.6%15.7%15.9%16.0%16.2%16.3%16.4%16.6%16.7%16.9%17.0%17.2%17.3%17.5%17.6%17.8%17.9%18.1%18.2%18.4%18.5%18.6%18.8%18.9%19.1%19.2%19.4%19.5%19.7%19.8%20.0%20.1%20.3%20.4%20.6%20.7%20.9%21.0%21.1%21.3%21.4%21.6%21.7%21.9%22.0%22.2%22.3%22.5%22.6%22.8%22.9%23.1%23.2%23.3%23.5%23.6%23.8%23.9%24.1%24.2%24.4%24.5%24.7%24.8%25.0%25.1%25.3%25.4%25.6%25.7%25.8%26.0%26.1%26.3%26.4%26.6%26.7%26.9%27.0%27.2%27.3%27.5%27.6%27.8%27.9%28.0%28.2%28.3%28.5%28.6%28.8%28.9%29.1%29.2%29.4%29.5%29.7%29.8%30.0%30.1%30.2%30.4%30.5%30.7%30.8%31.0%31.1%31.3%31.4%31.6%31.7%31.9%32.0%32.2%32.3%32.5%32.6%32.7%32.9%33.0%33.2%33.3%33.5%33.6%33.8%33.9%34.1%34.2%34.4%34.5%34.7%34.8%34.9%35.1%35.2%35.4%35.5%35.7%35.8%36.0%36.1%36.3%36.4%36.6%36.7%36.9%37.0%37.2%37.3%37.4%37.6%37.7%37.9%38.0%38.2%38.3%38.5%38.6%38.8%38.9%39.1%39.2%39.4%39.5%39.6%39.8%39.9%40.1%40.2%40.4%40.5%40.7%40.8%41.0%41.1%41.3%41.4%41.6%41.7%41.9%42.0%42.1%42.3%42.4%42.6%42.7%42.9%43.0%43.2%43.3%43.5%43.6%43.8%43.9%44.1%44.2%44.3%44.5%44.6%44.8%44.9%45.1%45.2%45.4%45.5%45.7%45.8%46.0%46.1%46.3%46.4%46.5%46.7%46.8%47.0%47.1%47.3%47.4%47.6%47.7%47.9%48.0%48.2%48.3%48.5%48.6%48.8%48.9%49.0%49.2%49.3%49.5%49.6%49.8%49.9%50.1%50.2%50.4%50.5%50.7%50.8%51.0%51.1%51.2%51.4%51.5%51.7%51.8%52.0%52.1%52.3%52.4%52.6%52.7%52.9%53.0%53.2%53.3%53.5%53.6%53.7%53.9%54.0%54.2%54.3%54.5%54.6%54.8%54.9%55.1%55.2%55.4%55.5%55.7%55.8%55.9%56.1%56.2%56.4%56.5%56.7%56.8%57.0%57.1%57.3%57.4%57.6%57.7%57.9%58.0%58.1%58.3%58.4%58.6%58.7%58.9%59.0%59.2%59.3%59.5%59.6%59.8%59.9%60.1%60.2%60.4%60.5%60.6%60.8%60.9%61.1%61.2%61.4%61.5%61.7%61.8%62.0%62.1%62.3%62.4%62.6%62.7%62.8%63.0%63.1%63.3%63.4%63.6%63.7%63.9%64.0%64.2%64.3%64.5%64.6%64.8%64.9%65.1%65.2%65.3%65.5%65.6%65.8%65.9%66.1%66.2%66.4%66.5%66.7%66.8%67.0%67.1%67.3%67.4%67.5%67.7%67.8%68.0%68.1%68.3%68.4%68.6%68.7%68.9%69.0%69.2%69.3%69.5%69.6%69.8%69.9%70.0%70.2%70.3%70.5%70.6%70.8%70.9%71.1%71.2%71.4%71.5%71.7%71.8%72.0%72.1%72.2%72.4%72.5%72.7%72.8%73.0%73.1%73.3%73.4%73.6%73.7%73.9%74.0%74.2%74.3%74.4%74.6%74.7%74.9%75.0%75.2%75.3%75.5%75.6%75.8%75.9%76.1%76.2%76.4%76.5%76.7%76.8%76.9%77.1%77.2%77.4%77.5%77.7%77.8%78.0%78.1%78.3%78.4%78.6%78.7%78.9%79.0%79.1%79.3%79.4%79.6%79.7%79.9%80.0%80.2%80.3%80.5%80.6%80.8%80.9%81.1%81.2%81.4%81.5%81.6%81.8%81.9%82.1%82.2%82.4%82.5%82.7%82.8%83.0%83.1%83.3%83.4%83.6%83.7%83.8%84.0%84.1%84.3%84.4%84.6%84.7%84.9%85.0%85.2%85.3%85.5%85.6%85.8%85.9%86.0%86.2%86.3%86.5%86.6%86.8%86.9%87.1%87.2%87.4%87.5%87.7%87.8%88.0%88.1%88.3%88.4%88.5%88.7%88.8%89.0%89.1%89.3%89.4%89.6%89.7%89.9%90.0%90.2%90.3%90.5%90.6%90.7%90.9%91.0%91.2%91.3%91.5%91.6%91.8%91.9%92.1%92.2%92.4%92.5%92.7%92.8%93.0%93.1%93.2%93.4%93.5%93.7%93.8%94.0%94.1%94.3%94.4%94.6%94.7%94.9%95.0%95.2%95.3%95.4%95.6%95.7%95.9%96.0%96.2%96.3%96.5%96.6%96.8%96.9%97.1%97.2%97.4%97.5%97.7%97.8%97.9%98.1%98.2%98.4%98.5%98.7%98.8%99.0%99.1%99.3%99.4%99.6%99.7%99.9%100.0%
2021-08-07 08:58:38,947-INFO: File /root/.paddlenlp/datasets/Glue/RTE.zip md5 checking...
2021-08-07 08:58:38,951-INFO: Decompressing /root/.paddlenlp/datasets/Glue/RTE.zip...
2021-08-07 08:58:38,986-INFO: unique_endpoints {'127.0.0.1:54561'}
W0807 08:58:38.990123   679 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.4, Runtime API Version: 11.2
W0807 08:58:38.993100   679 device_context.cc:422] device: 0, cuDNN Version: 8.1.
num_training_steps 1560
global step 100/1560, epoch: 0, batch: 99, rank_id: 0, loss: 0.560222, lr: 0.0000199046, speed: 12.2257 step/s
====================================================================================================
eval loss: 0.311436, acc: 0.8158844765342961, 
eval done total : 0.5279021263122559 s
global step 200/1560, epoch: 1, batch: 43, rank_id: 0, loss: 0.099705, lr: 0.0000185412, speed: 8.3867 step/s
====================================================================================================
eval loss: 0.253943, acc: 0.8447653429602888, 
eval done total : 0.5228173732757568 s
global step 300/1560, epoch: 1, batch: 143, rank_id: 0, loss: 0.253540, lr: 0.0000171779, speed: 8.4883 step/s
====================================================================================================
eval loss: 0.209711, acc: 0.8447653429602888, 
eval done total : 0.5178666114807129 s
global step 400/1560, epoch: 2, batch: 87, rank_id: 0, loss: 0.243290, lr: 0.0000158146, speed: 8.5058 step/s
====================================================================================================
eval loss: 0.320983, acc: 0.8483754512635379, 
eval done total : 0.5222113132476807 s
global step 500/1560, epoch: 3, batch: 31, rank_id: 0, loss: 0.024905, lr: 0.0000144513, speed: 8.3723 step/s
====================================================================================================
eval loss: 0.084361, acc: 0.851985559566787, 
eval done total : 0.5281729698181152 s
global step 600/1560, epoch: 3, batch: 131, rank_id: 0, loss: 0.075431, lr: 0.0000130879, speed: 8.5824 step/s
====================================================================================================
eval loss: 0.168876, acc: 0.8664259927797834, 
eval done total : 0.5221655368804932 s
global step 700/1560, epoch: 4, batch: 75, rank_id: 0, loss: 0.018467, lr: 0.0000117246, speed: 8.4658 step/s
====================================================================================================
eval loss: 0.118312, acc: 0.8592057761732852, 
eval done total : 0.6009185314178467 s
global step 800/1560, epoch: 5, batch: 19, rank_id: 0, loss: 0.299798, lr: 0.0000103613, speed: 8.5301 step/s
====================================================================================================
eval loss: 0.255902, acc: 0.8375451263537906, 
eval done total : 0.5747411251068115 s
global step 900/1560, epoch: 5, batch: 119, rank_id: 0, loss: 0.010474, lr: 0.0000089980, speed: 8.6720 step/s
====================================================================================================
eval loss: 0.231587, acc: 0.8447653429602888, 
eval done total : 0.5697014331817627 s
global step 1000/1560, epoch: 6, batch: 63, rank_id: 0, loss: 0.030244, lr: 0.0000076346, speed: 8.5721 step/s
====================================================================================================
eval loss: 0.412517, acc: 0.855595667870036, 
eval done total : 0.5694811344146729 s
global step 1100/1560, epoch: 7, batch: 7, rank_id: 0, loss: 0.007879, lr: 0.0000062713, speed: 8.6421 step/s
====================================================================================================
eval loss: 0.312517, acc: 0.851985559566787, 
eval done total : 0.5717041492462158 s
global step 1200/1560, epoch: 7, batch: 107, rank_id: 0, loss: 0.006305, lr: 0.0000049080, speed: 8.6437 step/s
====================================================================================================
eval loss: 0.326864, acc: 0.8447653429602888, 
eval done total : 0.5699319839477539 s
global step 1300/1560, epoch: 8, batch: 51, rank_id: 0, loss: 0.006313, lr: 0.0000035446, speed: 8.6407 step/s
====================================================================================================
eval loss: 0.505613, acc: 0.851985559566787, 
eval done total : 0.5628728866577148 s
global step 1400/1560, epoch: 8, batch: 151, rank_id: 0, loss: 0.005310, lr: 0.0000021813, speed: 8.7495 step/s
====================================================================================================
eval loss: 0.287311, acc: 0.8483754512635379, 
eval done total : 0.5722286701202393 s
global step 1500/1560, epoch: 9, batch: 95, rank_id: 0, loss: 0.005108, lr: 0.0000008180, speed: 8.6264 step/s
====================================================================================================
eval loss: 0.350723, acc: 0.8447653429602888, 
eval done total : 0.5756144523620605 s
global step 1560/1560, epoch: 9, batch: 155, rank_id: 0, loss: 0.005336, lr: 0.0000000000, speed: 11.8593 step/s
====================================================================================================
eval loss: 0.353380, acc: 0.8447653429602888, 
eval done total : 0.5736029148101807 s
